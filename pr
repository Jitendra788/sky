import requests
from bs4 import BeautifulSoup
import os
import pandas as pd
from PIL import Image
import time
import json
import re
from selenium import webdriver
from urllib.parse import urlparse
from selenium_stealth import stealth
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urljoin
import uuid
import sys
import logging
import concurrent.futures
import bson
import random
from dotenv import load_dotenv
load_dotenv()
import mysql.connector

conn = mysql.connector.connect(
    host="localhost",
    user="root",
    password="root",
    database="business_db"
)
cursor = conn.cursor(buffered=True)
class jewellery  :
    headers1={
            'authority': 'www.google.com',
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'accept-language': 'en-US,en;q=0.9',
                'cache-control': 'max-age=0',
                'sec-ch-ua': '"Chromium";v="118", "Google Chrome";v="118", "Not=A?Brand";v="99"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"Windows"',
                'sec-fetch-dest': 'document',
                'Accept-Encoding':'gzip, deflate, br',
                'sec-fetch-mode': 'navigate',
                'sec-fetch-site': 'none',
                'sec-fetch-user': '?1',
                'upgrade-insecure-requests': '1',
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'
        }
    def get_random_user_agent(self):
        random_number = random.randint(100000000, 90000000000000)
        user_agents = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"+str(random_number)
        return user_agents
    def get_rotating_Headers(self):
        agent=self.get_random_user_agent()
        headers = {
                'authority': 'www.google.com',
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'accept-language': 'en-US,en;q=0.9',
                'cache-control': 'max-age=0',
                'sec-ch-ua': '"Chromium";v="118", "Google Chrome";v="118", "Not=A?Brand";v="99"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"Windows"',
                'sec-fetch-dest': 'document',
                'Accept-Encoding':'gzip, deflate, br',
                'sec-fetch-mode': 'navigate',
                'sec-fetch-site': 'none',
                'sec-fetch-user': '?1',
                'upgrade-insecure-requests': '1',
                'user-agent': agent
            }                     
        return headers

 
    def __init__(self,url,catID,subCatId) -> None:  
        self.categoryId=catID
        self.subCatId=subCatId
        print(self.categoryId+'-'+url)
        options = Options()
        # options.add_argument("--headless")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-popup-blocking')
        options.add_argument('start-maximized')
        options.add_argument('disable-infobars')
        options.add_argument('--profile-directory=Default')
        options.add_argument("--incognito")
        options.add_argument("--disable-plugins-discovery")
        options.add_experimental_option("excludeSwitches", ["ignore-certificate-errors", "safebrowsing-disable-download-protection", "safebrowsing-disable-auto-update", "disable-client-side-phishing-detection"])
        options.add_argument('--disable-extensions')
        options.add_argument('--start-maximized')
        options.add_argument('--disable-extensions')
        # options.add_argument(f'--proxy-server={strome_proxy}')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        s = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=s, options=options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        stealth(self.driver,
            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            languages=["en-US", "en"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True,
            )
        self.count_x = 1
        self.urls = url

    def crawl1(self,city): 
        
        page_size=0 
        while(True):                         
            try:
                print('Requesting....................') 
                
                link = f"{self.urls}start:{page_size}"
                print(link)
                self.driver.get(link)
                time.sleep(10)
                page_source = self.driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')
                a_tags = soup.find_all('a', class_='vwVdIc')
                links = []
                for a_tag in a_tags:
                    data_cid_value = a_tag.get('data-cid')
                    if data_cid_value not in links:
                        links.append(data_cid_value)
                    print(f'data-cid: {data_cid_value}')
                
                
                print(len(links))
                if len(links) == 0:
                    print("complete---")
                    break
                for link in links:               
                        self.parse(link,city)
                page_size += 20

            except Exception as e: 
                print(e)                         
                return 'nones'
            
    
    def is_excluded_domain(self,url, exclusion_list):
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        return any(excluded_domain in domain for excluded_domain in exclusion_list) 
    def add_domain_to_url(self,url, base_domain):
        if urlparse(url).netloc == '':
            return urljoin(base_domain, url)
        return url      
    def parse(self,url_id,city):
        try:
            url_template = (
                "https://www.google.com/search?q=jewellery+shop+{city}"
                "&sca_esv=587921074&biw=1229&bih=566&tbm=lcl"
                "&sxsrf=AM9HkKnfh9ZjcGViguw5FfgJi2AljInJtA%3A1701754671012"
                "&ei=L7duZewe8uPaug-RuqXYDA&ved=0ahUKEwis8qOVyveCAxXysVYBHRFdCcsQ4dUDCAk"
                "&uact=5&oq=jewellery+shop+{city}&gs_lp=Eg1nd3Mtd2l6LWxvY2FsIhtqZXdlbGxlcnkgc2hvcCBpbiBTYW4gRGllZ28yBBAjGCdI-fcSUIjgEljj8RJwAXgAkAEAmAGyAaABhASqAQMwLjO4AQPIAQD4AQL4AQHCAgoQABiABBiKBRhDwgIGEAAYFhgewgIHECMYsAIYJ4gGAQ"
                "&sclient=gws-wiz-local#rlfi=hd:;si:{url_id},l,ChtqZXdlbGxlcnkgc2hvcCBpbiBTYW4gRGllZ29Ii_7lr-aAgIAIWjEQABABGAAYARgDGAQiG2pld2VsbGVyeSBzaG9wIGluIHNhbiBkaWVnbyoGCAMQABABkgENamV3ZWxyeV9zdG9yZZoBI0NoWkRTVWhOTUc5blMwVkpRMEZuU1VOU09YSjFaR04zRUFFqgFiCggvbS8wNDA0ZBABKhIiDmpld2VsbGVyeSBzaG9wKAAyHxABIhvwTrOjpVYz-xNQ-2KkQ3Zqk-By0GAMaaZruHwyHxACIhtqZXdlbGxlcnkgc2hvcCBpbiBzYW4gZGllZ28,y,Jx7fkb0U9s8;mv:[[32.8808492,-117.12227259999997],[32.7004264,-117.25716700000001]];"
            )

            url = url_template.format(city=city, url_id=url_id) 
            print(' ')
            logging.info(' ')
            print("processing product :- "+url)
            business_name=''
            retry=0
            while retry<=10:
                try:
                    # response = requests.get(url,headers=self.get_rotating_Headers())
                    self.driver.get(url)
                    time.sleep(10)
                    page_source = self.driver.page_source
                    soup = BeautifulSoup(page_source, 'html.parser')
                    # file_path="C:\\Users\\jiten\\Downloads\\output.html"
                    # with open(file_path, 'w', encoding='utf-8') as file:
                    #     file.write(soup.prettify())
                    business_element = soup.select_one('.SPZz6b>h2>span')
                    if business_element:
                        business_name = business_element.text
                        print("business_name--",business_name)
                        break
                except Exception as e:
                    print('retry :- '+str(retry))
                    retry+=1
            address_element = soup.find('span', class_='LrzXr')
            phone_element = soup.select('span.kno-fv>a>span')
            phone_no = phone_element[0].text
            address_name = address_element.text.strip()
            split_address = address_name.split(', ')
            country = split_address[-1] 
            email=''
            cite_tag = soup.find('cite', class_='gr1Yld')
            if cite_tag:
                url = cite_tag.get_text()
                print(f'URL: {url}')
            a_tag=''
            a_tag = soup.find('a', class_='GFNUx')
            if a_tag:
                url = a_tag['href']
                print(f'URL: {url}')
            Website_url=''
            all_a_tags=''
            try:
                if a_tag:
                    Website_url = a_tag['href']
                    print(f'URL: {Website_url}')
                    excluded_domains = ['facebook.com', 'instagram.com', 'linkedin.com']
                    if not self.is_excluded_domain(Website_url, excluded_domains):
                        try:
                            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                            response = requests.get(Website_url, headers=self.get_rotating_Headers())
                            soup = BeautifulSoup(response.text, 'html.parser')
                            text_nodes = soup.find_all(string=True, recursive=True)
                            email_found = False
                            for text_node in text_nodes:
                                email_matches = re.findall(email_pattern, text_node)
                                for email in email_matches:
                                    print(f'Email: {email}')
                                    email_found = True
                                    break
                                if email_found:
                                    break

                            if not email_found:
                                all_a_tags = soup.find_all('a')
                            for a_tag in all_a_tags:
                                href = a_tag.get('href')
                                if 'about' in href or 'contact' in href or href.startswith('http'):
                                    href = self.add_domain_to_url(href, Website_url)
                                    response = requests.get(href, headers=self.get_rotating_Headers())
                                    inner_soup = BeautifulSoup(response.text, 'html.parser')
                                    email_matches = re.findall(email_pattern, inner_soup.get_text())
                                    for email in email_matches:
                                        if email:  
                                            print(f'Email: {email}')
                                            email_found = True
                                            break
                                    if email_found:
                                        break

                        except Exception as e:
                            print(f"Error: {e}")

                    else:
                        print(f'Skipping {Website_url} - Excluded domain detected.')
                else:
                    print('Anchor tag not found.')
            except Exception as e:
                print(e)
            cursor.execute(
                "INSERT INTO business(business_name,country,phone_no,email,address_name, websit_url) VALUES (%s,%s,%s, %s,%s,%s)",(business_name, country,phone_no,email,address_name,Website_url)
            )
            conn.commit()
            # cursor.close()
            # conn.close()
        except Exception as e:
            print(e)
                         

if __name__ == '__main__':
    # arguments = sys.argv[1:]
    # if len(arguments) > 0:
    #     dynamic_url = sys.argv[1]
    #     catID = sys.argv[2]
    #     subCatId=sys.argv[3]
    city_list_str = os.getenv("CITY_LIST")
    if city_list_str:
        list_city = city_list_str.split(',')
        base_url = "https://www.google.com/search?q=jewellery+shop+{}&sca_esv=587921074&biw=1229&bih=566&tbm=lcl&sxsrf=AM9HkKmGkbuIDc2R765oRbNZseuzkKSm-A%3A1701937851819&ei=u4JxZfjOMdXO1e8PjqC3oAo&ved=0ahUKEwi4mNnI9PyCAxVVZ_UHHQ7QDaQQ4dUDCAk&uact=5&oq=jewellery+shop+in+San+Jose&gs_lp=Eg1nd3Mtd2l6LWxvY2FsIhpqZXdlbGxlcnkgc2hvcCBpbiBTYW4gSm9zZTIEECMYJzIGEAAYFhgeMgYQABgWGB4yBhAAGBYYHjIGEAAYFhgeMgYQABgWGB4yBhAAGBYYHjIGEAAYFhgeMgsQABiABBiKBRiGAzILEAAYgAQYigUYhgNI9ilQmAVYmAVwAHgAkAEAmAGFAqAB-QOqAQMyLTK4AQPIAQD4AQL4AQGIBgE&sclient=gws-wiz-local#rlfi=hd:;si:;mv:[[37.3613676,-121.803332],[37.2429278,-121.9987742]];"
        for city in list_city:
            formatted_url = base_url.format(city)
            jewellery_obj = jewellery(formatted_url,'catID','subCatId')
            jewellery_obj.crawl1(city)
            jewellery_obj.driver.close()
            jewellery_obj.driver.quit()
            print(formatted_url)
    else:
        print("CITY_LIST environment variable not set.")

    
   
